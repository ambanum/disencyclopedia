---
parent: Reaction
title: Case studies
nav_order: 1
permalink: /encyclopedia/reaction/case-studies
---

# Reaction case studies

- - -

### Table of contents
{: .no_toc.no-numbering.toc-title}

1. Table of contents
{:toc}

- - -

## Legislation

### The French law against information manipulation

In France, a law against information manipulation and the **intentional** spread of disinformation was officially adopted on November 20, 2018, nine months after its proposal.

<!-- add decree publication date -->

It focuses on massive and rapid spreads of disinformation on social media and more widely digital tools, including foreign state-owned media outlets, and was meant to be especially efficient in electoral contexts.

During said times, the law compels platforms to be more transparent by, for instance, reporting sponsored content as well as the name and invested resources of sponsors. The biggest, most used platforms are to have a legal representative in France and publish their algorithms as open data.

"Fake news" are also formally defined in order to allow for judiciary action. This definition includes massive and artificial diffusion, capacity of disturbance (of public peace or elections), and undeniability of the falsehood of the information being spread.

Outside of election times, platforms ought to cooperate, as well as build and implement open measures to fight disinformation.
The French Superior Council of the Audiovisual (CSA) has been given the authority to hinder or interrupt the broadcasting of foreign state-owned (or state-influenced) TV-services in cases where fundamental national interests are at stake.


### The EU Code of Practice on Disinformation

The Code of Practice was impulsed by the **European Commission**. Signed in September 2018 and entered into force one month later, the Code was developed to achieve objectives already laid out in April of the same year regarding the **spread of disinformation online**, especially on social media platforms ahead of the European elections in May 2019.

Commitment to and implementation of the code work on a **voluntary basis** with **self-regulatory standards**.

#### Signatories

Singatories include platforms such as Facebook, Twitter, YouTube, but also Google & Mozilla. Beside representatives of online platforms and prominent social networks are advertising industry actors.

#### Scope

The application of the Code of Practices is limited for each signatory to services provided within the European Economic Area (EEA).

#### Goal

Overall, signatories must contribute to solutions to challenges raised by disinformation. As explained in the text itself, "_the purpose of this Code is to identify the actions that Signatories could put in place in order to address the challenges related to "Disinformation"_".

Among these actions are efforts towards more transparency, safeguards, scrutiny, a reduced visibility of fake information, an improvement of  the findability of trustworthy content, among others.

#### Defining disinformation

The Code offers a rather complete definition of disinformation, on which all signatories agree. Excluding "_misleading advertising, reporting errors, satire and parody, or clearly identified partisan news and commentary_", disinformation is defined as "**_verifiably false or misleading information_**" that is "_created, presented and disseminated for **economic gain** or to **intentionally deceive** the public_" ; and that "_may cause **public harm**_" and threatens "_**democratic political and policymaking processes** as well as **public goods** such as the protection of EU citizens' health, the environment or security_".

#### Commitments

The Code lists a variety of detailed measures to which signatories commit, according to their technical capabilities, the services they provide, their liability regime and "_the role they play in the creation and dissemination of the content at stake_", among other criteria.

Said measures include the **scrutiny of advertising placements** (for instance,  the deployment of policies and processes disrupting "_advertising and monetization incentives for relevant behaviours_") ; the clear and public **disclosure of political advertising and issue-based advertising** (in order to distinguish them from editorial content like news ) ; as well as the **measuring and monitoring of the effectiveness of the Code**.

Moreover, signatories commit to make efforts to **empower consumers and the research community**. Most also engage themselves to putting in place and enforce "_clear policies regarding identity and the misuse of automated bots on their services_".

To help signatories do so, best practices are detailed in the annex of the Code. However, considering the diverse nature of the signatories' operations, purposes, technologies and audiences, the Code is open to various other approaches "_accomplishing the spirit of [its] provisions_".

## Correct the record

On March 12, 2019, international cyberactivism NGO Avaaz ("voice" in sanskrit) published a report called "Yellow Vests flooded by Fake News - Over 100M views if disinformation on Facebook" in which it called on Facebook to "**Correct the Record**" ahead of EU Elections.
Alongside a study on disinformation - especially Russian campaigns - focused on the Yellow Vests movement in France, Avaaz details its proposal for an innovative solution.

Based on cooperation between factcheckers and platforms (especially Facebook), the initiative works following a [five-step process](https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/AVAAZ_YellowVests_100miofake.pdf.pdf.pdf).
First,  content **viewed by a significant number of people** and deemed false or misleading after **verification by independent fact-checkers** would "activate" an obligation for platforms to correct the record.
Then, platforms woud have to provide a **misinformation report mechanism** that is easy to access for users, as well as access for fact-checkers to content that has been viewed by a significant number of people.
Reported content should then be **fact-checked** within 24 hours by "_independent, third-party, verified factcheckers_" working with the platforms.
Platforms are also asked to display their "_most visible notification standard_" to **notify** all users exposed to verified
disinformation.
Lastly,  "_each user exposed to disinformation should receive a correction that is of at least equal prominence to the original content and that follows best practices_", specifically adapted to the user's profile.

Ultimately, all platforms users exposed to disinformation would receive independent third party corrections. One key-point is the avoidance of the repetition of disinformation.

Through this initiative, Avaaz's goal is to "_restore the publicâ€™s trust_" and "_ensure the integrity_" of the upcoming European elections.

More information as to the usefulness and effectiveness of correction in hindering the effects of disinformation can be found [here](https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/AVAAZ_YellowVests_100miofake.pdf.pdf.pdf) (p.14/28), [here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/112200/jcom12164.pdf?sequence=1&isAllowed=y) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5383823/).
